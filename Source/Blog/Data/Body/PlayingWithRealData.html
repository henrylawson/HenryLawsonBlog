<p>Recently I was apart of a software project that was creating a replacement to a legacy application. This legacy application <strong>contained a significant but not not outrageous amount of data</strong>. As part of this replacement we had to migrate all of the existing data into our new data model. This task was <strong>thoughtlessly left until the end of the project</strong>, an iteration before the release. The reasoning for this decision was that it wasn't seen as a big overhead and that we were <strong>testing our system with production like quantities of data</strong>. That is, our generated data had around the same amount of records that the legacy application had for certain entities. The following issues were observed once the data was completely migrated:</p>
<ol>
	<li>Certain areas of the application performed terribly</li>
	<li>The legacy data did not contain various records that we expected to exist</li>
	<li>Our search re-indexing system performed worse</li>
</ol>
<p>Now, two of the above points required a bit of re-work, <i>re-work that probably could of been addressed and identified much earlier in the project</i>. <strong>The root cause of points one and three was that the complexity of our generated data did not match that of our production data. We didn't have the complex relationships that the "real" data had</strong>. Our generated data was not derived from production data. It was generated by hand and then duplicated until the record count matched that of what we noted in production. I don't think there is anything wrong with that approach, it is certainly common practice and the simplest way to get a large data set to test against. <strong>What we didn't realize was the impact that data with more complex one-to-one or one-to-many relationships would have on certain features of our system</strong>.</p>
<p>The most obvious solution to this problem, which is considered an anti-pattern is to, <i>start using production data as soon as possible</i>. The reasons for why this is an anti-pattern are:</p>
<ol>
	<li>Privacy and confidentiality issues.</li>
	<li>Some production databases are just too big to replicate across multiple development environments.</li>
	<li>Production data often becomes stale and people will typically want to "refresh" it on development environments. This results in the assumptions being made about the old data invalid. New assumptions about how the system under test should perform with the new data are then made. This ambiguity in assumptions leads to incorrect decisions and over optimization of perceived problem areas that go away after each refresh.</li>
	<li>The need to use production data highlights a lack of understanding or lack of desire to understand the complex structure of the data itself.</li>
</ol>
<p>Problems one and two can be addressed by using <i>Production Like Data</i>. <i>Production Like Data</i> is production data that has been obfuscated to ensure privacy and may also be a smaller but still meaningful subset of the production data. <storng><i>Production Like Data</i> should always be used where production data is needed. However, <i>Production Like Data</i> shouldn't be used everywhere:</p>
<ul>
	<li><i>Production Like Data</i> should not be used for acceptance tests</li>
	<li><i>Production Like Data</i> should only be used for exploratory, performance or upgrade testing</li>
</ul>
<p><srong>Using <i>Production Like Data</i> in acceptance tests really highlights a lack of understanding the data</strong>, which is what problem three and four above are really about. A priority of acceptance tests should be to test the functionality with the <i>least amount of setup and overhead as possible</i>. This is to minimize the scope and complexity of what is being tested. <strong>Relying on <i>Production Like Data</i> violates this priority and will result in flaky difficult to manage tests</strong>. Especially as there should not and probably won't be, any control on the state or content of the <i>Production Like Data</i>.</p>
<p>For the remaining three forms of testing, <i>Production Like Data</i> is typically seen as OK. The reasoning behind this is that it is difficult to reach a sufficient level of confidence about the system without data that mimics that which exists in the wild.</p>